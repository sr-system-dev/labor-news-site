#!/usr/bin/env python3
"""
åŠ´å‹™é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã€Markdownå½¢å¼ã§ä¿å­˜ã™ã‚‹

ä½¿ç”¨æ–¹æ³•:
    python collect_news.py           # éå»7æ—¥é–“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    python collect_news.py --days 14 # éå»14æ—¥é–“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†
"""

import argparse
import feedparser
from datetime import datetime, timedelta
from pathlib import Path
import html
import re
from typing import NamedTuple
from collections import defaultdict


class NewsItem(NamedTuple):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    title: str
    link: str
    published: datetime
    summary: str
    source: str


# RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®è¨­å®š
RSS_FEEDS = {
    "åšç”ŸåŠ´åƒçœ": "https://www.mhlw.go.jp/stf/rss/shinchaku.xml",
    "åšç”ŸåŠ´åƒçœï¼ˆå ±é“ç™ºè¡¨ï¼‰": "https://www.mhlw.go.jp/stf/rss/houdou.xml",
    "åŠ´åƒæ–°èç¤¾": "https://www.rodo.co.jp/feed/",
    "åŠ´å‹™ãƒ‰ãƒƒãƒˆã‚³ãƒ ": "https://roumu.com/feed/",
    "æ—¥æœ¬ã®äººäº‹éƒ¨": "https://jinjibu.jp/rss/news.rss",
}

# åŠ´å‹™é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ï¼‰
LABOR_KEYWORDS = [
    "åŠ´åƒ", "é›‡ç”¨", "è³ƒé‡‘", "çµ¦ä¸", "æ®‹æ¥­", "åƒãæ–¹", "åŠ´å‹™",
    "äººäº‹", "æ¡ç”¨", "é€€è·", "è§£é›‡", "ä¼‘æš‡", "æœ‰çµ¦", "è‚²å…",
    "ä»‹è­·", "ãƒãƒ©ã‚¹ãƒ¡ãƒ³ãƒˆ", "ãƒ‘ãƒ¯ãƒãƒ©", "ã‚»ã‚¯ãƒãƒ©", "åŠ´ç½",
    "ç¤¾ä¼šä¿é™º", "åšç”Ÿå¹´é‡‘", "å¥åº·ä¿é™º", "é›‡ç”¨ä¿é™º", "åŠ´åƒåŸºæº–",
    "æœ€ä½è³ƒé‡‘", "åŒä¸€åŠ´åƒ", "ãƒ†ãƒ¬ãƒ¯ãƒ¼ã‚¯", "åœ¨å®…å‹¤å‹™", "å‰¯æ¥­",
    "å…¼æ¥­", "å®šå¹´", "å†é›‡ç”¨", "æ´¾é£", "å¥‘ç´„ç¤¾å“¡", "æ­£ç¤¾å“¡",
    "éæ­£è¦", "å°±æ¥­è¦å‰‡", "åŠ´åƒçµ„åˆ", "å›£ä½“äº¤æ¸‰", "ã‚¹ãƒˆãƒ©ã‚¤ã‚­",
    "36å”å®š", "å®‰å…¨è¡›ç”Ÿ", "ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ˜ãƒ«ã‚¹", "éåŠ´", "é•·æ™‚é–“åŠ´åƒ",
]

# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
OUTPUT_DIR = Path("news")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åé›†æ—¥æ•°
DEFAULT_DAYS = 7


def clean_html(text: str) -> str:
    """HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    if not text:
        return ""
    # HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    text = html.unescape(text)
    # HTMLã‚¿ã‚°ã‚’é™¤å»
    text = re.sub(r'<[^>]+>', '', text)
    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def parse_date(entry: dict) -> datetime:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰æ—¥ä»˜ã‚’è§£æ"""
    # published_parsedãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
    if hasattr(entry, 'published_parsed') and entry.published_parsed:
        return datetime(*entry.published_parsed[:6])
    # updated_parsedãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
    if hasattr(entry, 'updated_parsed') and entry.updated_parsed:
        return datetime(*entry.updated_parsed[:6])
    # ã©ã¡ã‚‰ã‚‚ãªã‘ã‚Œã°ç¾åœ¨æ™‚åˆ»
    return datetime.now()


def is_labor_related(text: str) -> bool:
    """ãƒ†ã‚­ã‚¹ãƒˆãŒåŠ´å‹™é–¢é€£ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    text_lower = text.lower()
    return any(keyword in text_lower for keyword in LABOR_KEYWORDS)


def fetch_feed(url: str, source_name: str) -> list[NewsItem]:
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    items = []
    try:
        feed = feedparser.parse(url)

        if feed.bozo and not feed.entries:
            print(f"  è­¦å‘Š: {source_name} ã®ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")
            return items

        for entry in feed.entries:
            title = clean_html(entry.get('title', ''))
            link = entry.get('link', '')
            summary = clean_html(entry.get('summary', entry.get('description', '')))
            published = parse_date(entry)

            # ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯ã‚µãƒãƒªãƒ¼ãŒåŠ´å‹™é–¢é€£ã®å ´åˆã®ã¿è¿½åŠ 
            # åšç”ŸåŠ´åƒçœã®ãƒ•ã‚£ãƒ¼ãƒ‰ã¯å…¨ã¦å«ã‚ã‚‹
            if "åšç”ŸåŠ´åƒçœ" in source_name or is_labor_related(title + summary):
                items.append(NewsItem(
                    title=title,
                    link=link,
                    published=published,
                    summary=summary[:200] + "..." if len(summary) > 200 else summary,
                    source=source_name,
                ))
    except Exception as e:
        print(f"  ã‚¨ãƒ©ãƒ¼: {source_name} ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ - {e}")

    return items


def filter_by_date_range(
    items: list[NewsItem], start_date: datetime, end_date: datetime
) -> list[NewsItem]:
    """æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ç¯„å›²å†…ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    filtered = []
    for item in items:
        item_date = item.published.replace(hour=0, minute=0, second=0, microsecond=0)
        if start_date <= item_date <= end_date:
            filtered.append(item)
    return filtered


def group_by_date(items: list[NewsItem]) -> dict[str, list[NewsItem]]:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ—¥ä»˜ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
    grouped = defaultdict(list)
    for item in items:
        date_str = item.published.strftime("%Y-%m-%d")
        grouped[date_str].append(item)
    return grouped


def generate_markdown(
    items: list[NewsItem], start_date: datetime, end_date: datetime
) -> str:
    """é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”¨ã®Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ"""
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")

    lines = [
        f"# åŠ´å‹™é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹",
        f"## æœŸé–“: {start_str} ã€œ {end_str}",
        "",
        f"*åé›†æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M')}*",
        "",
        f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: **{len(items)}ä»¶**",
        "",
        "---",
        "",
    ]

    # æ—¥ä»˜ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    by_date = group_by_date(items)

    for date_str in sorted(by_date.keys(), reverse=True):
        date_items = by_date[date_str]
        lines.append(f"## ğŸ“… {date_str}ï¼ˆ{len(date_items)}ä»¶ï¼‰")
        lines.append("")

        # ã‚½ãƒ¼ã‚¹ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        by_source = defaultdict(list)
        for item in date_items:
            by_source[item.source].append(item)

        for source, source_items in sorted(by_source.items()):
            lines.append(f"### {source}")
            lines.append("")

            for item in sorted(source_items, key=lambda x: x.published, reverse=True):
                time_str = item.published.strftime("%H:%M")
                lines.append(f"- [{item.title}]({item.link}) *({time_str})*")
                if item.summary:
                    # ã‚µãƒãƒªãƒ¼ã‚’çŸ­ãè¡¨ç¤º
                    short_summary = item.summary[:100] + "..." if len(item.summary) > 100 else item.summary
                    lines.append(f"  > {short_summary}")
            lines.append("")

        lines.append("---")
        lines.append("")

    return "\n".join(lines)


def save_markdown(start_date: datetime, end_date: datetime, content: str) -> Path:
    """Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ï¼ˆæ—¥ä»˜ç¯„å›²å½¢å¼ï¼‰"""
    OUTPUT_DIR.mkdir(exist_ok=True)
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")
    file_path = OUTPUT_DIR / f"{start_str}_{end_str}.md"
    file_path.write_text(content, encoding="utf-8")
    return file_path


def parse_args():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹"""
    parser = argparse.ArgumentParser(
        description="åŠ´å‹™é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ã—ã¦Markdownå½¢å¼ã§ä¿å­˜ã—ã¾ã™"
    )
    parser.add_argument(
        "--days",
        type=int,
        default=DEFAULT_DAYS,
        help=f"åé›†ã™ã‚‹æ—¥æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {DEFAULT_DAYS}æ—¥é–“ï¼‰",
    )
    return parser.parse_args()


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    args = parse_args()
    days = args.days

    # æ—¥ä»˜ç¯„å›²ã‚’è¨ˆç®—ï¼ˆä»Šæ—¥ã‚’å«ã‚€éå»Næ—¥é–“ï¼‰
    end_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    start_date = end_date - timedelta(days=days - 1)

    print("=" * 50)
    print("åŠ´å‹™é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 50)
    print()
    print(f"åé›†æœŸé–“: {start_date.strftime('%Y-%m-%d')} ã€œ {end_date.strftime('%Y-%m-%d')}ï¼ˆ{days}æ—¥é–“ï¼‰")
    print()

    all_items = []

    # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
    for source_name, url in RSS_FEEDS.items():
        print(f"å–å¾—ä¸­: {source_name}...")
        items = fetch_feed(url, source_name)
        print(f"  â†’ {len(items)}ä»¶å–å¾—")
        all_items.extend(items)

    print()
    print(f"ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰åˆè¨ˆ: {len(all_items)}ä»¶å–å¾—")

    # æ—¥ä»˜ç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_items = filter_by_date_range(all_items, start_date, end_date)
    print(f"æœŸé–“å†…ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(filtered_items)}ä»¶")
    print()

    if not filtered_items:
        print("æŒ‡å®šæœŸé–“å†…ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦1ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    print("Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­...")
    content = generate_markdown(filtered_items, start_date, end_date)
    file_path = save_markdown(start_date, end_date, content)
    print(f"  â†’ {file_path}")

    print()
    print("å®Œäº†ã—ã¾ã—ãŸï¼")


if __name__ == "__main__":
    main()
